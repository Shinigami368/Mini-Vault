MiniVault Bootstrap

> âš¡ This project was originally created as a case study to simulate infrastructure readiness for a GPU-powered AI workload.

Project Purpose:

This project simulates the initial bootstrap process of a local AI appliance. It checks whether the system has the necessary software and hardware components, runs a mock inference container, and logs each step in both plain text and structured JSON format.

Requirements:

	-Ubuntu 22.04 LTS operating system

	-Docker installed and running

	-Bash shell (default on most systems)

	-NVIDIA GPU with CUDA (Optional) 

	-Docker permissions configured (use sudo if needed)

Files:

	-diagnose.sh: Checks system readiness and generates logs

	-run_inference_stub.sh: Builds and runs the inference container

	-Dockerfile: Defines a minimal container that simulates a model

	-entrypoint.sh: Executes inside the container, mimics inference

	-input.json: Sample input to simulate model prompt

	-system_report.log: Human-readable log output

	-system_report.jsonl: Structured JSON log of diagnostics

	-inference.jsonl: Structured log generated by the container

	-output.json: Output file created by the fake inference

	-gpu_health.sh: Monitors and logs GPU temperature and memory usage (Bonus)

	-telemetry_server.py: Mock HTTP server that accepts logs via POST (Bonus)

	-systemd/inference-stub.service: Systemd unit file to auto-start the inference container at boot (Bonus)

How to Use:

	Make the scripts executable:
	
		chmod +x diagnose.sh run_inference_stub.sh entrypoint.sh gpu_health.sh

	Run the system diagnostic:

		./diagnose.sh
		
	Create an input.json file like this:

		{
		  "model": "test-model-v1",
		  "prompt": "Hello world",
		  "parameters": {
			"max_tokens": 100,
			"temperature": 0.7
		  }
		}
		
	Run the inference container:

		./run_inference_stub.sh

	You should see the following outputs:

		system_report.log and system_report.jsonl and inference.log and inference.jsonl and output.json

	Run the GPU health monitor (optional):

		./gpu_health.sh

	Run the mock telemetry server (optional):

		pip install flask
		python3 telemetry_server.py

	Send logs via HTTP POST (optional):

		while IFS= read -r line; do
		  curl -X POST -H "Content-Type: application/json" \
		       -d "$line" http://localhost:5000/log
		done < inference.jsonl

	Set up systemd service (optional):

		sudo cp systemd/inference-stub.service /etc/systemd/system/
		sudo systemctl daemon-reexec
		sudo systemctl enable inference-stub
		sudo systemctl start inference-stub

Assumptions and Design Decisions:

	-No real model is loaded; this is a simulated environment.

	-The system gracefully handles missing GPUs.

	-All logs are written both as human-readable logs and JSON lines (.jsonl) for automated parsing.

	-Container interaction is based on mounted input/output files.

	-The goal is clarity and simplicity, not full-scale deployment.

Future Improvements:

	-Replace the stub with a real LLM or image model via Python or REST API

	-Add GPU health metrics like temperature and memory usage

	-Add a systemd service file for automatic startup at boot

	-Implement a local HTTP server to collect logs via POST

Notes:

	-If testing inside a virtual machine, GPU passthrough may not be available. The diagnostic script will log a warning, not an error.

	-Docker might require sudo depending on your system. Make sure your user has the necessary permissions.
